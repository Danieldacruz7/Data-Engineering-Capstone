{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8wxIYjBu_7O"
      },
      "source": [
        "### Analyse search terms on the e-commerce web server\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgNUSVQDIddi"
      },
      "source": [
        "##### In this assignment you will download the search term data set for the e-commerce web server and run analytic queries on it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKx6l0evIddi",
        "outputId": "4d9ce8cc-9b1f-4fe6-ca5d-baa4137b6cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 28 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 48.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=548d8832bba8990c3c92d0c7d48c28784d5394513e9270bf6b9020be86db77e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
            "Collecting pyspark2pmml\n",
            "  Downloading pyspark2pmml-0.5.1.tar.gz (1.6 kB)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.7/dist-packages (from pyspark2pmml) (0.10.9.3)\n",
            "Building wheels for collected packages: pyspark2pmml\n",
            "  Building wheel for pyspark2pmml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark2pmml: filename=pyspark2pmml-0.5.1-py3-none-any.whl size=2418 sha256=3056485987015e765a51075252e5f12838137b9f02a1683fb04f22dae0eb33b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/31/a6/934cee5fbba108e21ebabccb4b79b088cf550455e396447790\n",
            "Successfully built pyspark2pmml\n",
            "Installing collected packages: pyspark2pmml\n",
            "Successfully installed pyspark2pmml-0.5.1\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.7/dist-packages (0.10.9.3)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "# Install spark\n",
        "!pip install pyspark\n",
        "!pip install pyspark2pmml\n",
        "!pip install py4j \n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRU71ejbIddk"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_b7viT-Iddl"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fviLXPykIddm"
      },
      "outputs": [],
      "source": [
        "# Creating a spark context class\n",
        "sc = SparkContext()\n",
        "\n",
        "# Creating a spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Saving and Loading a SparkML Model\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQcg6l5_Iddm",
        "outputId": "b7de388e-758b-4a2a-8f47-9e4fed8eb834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-06 12:10:13--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/searchterms.csv\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 233457 (228K) [text/csv]\n",
            "Saving to: ‘searchterms.csv’\n",
            "\n",
            "searchterms.csv     100%[===================>] 227.99K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-03-06 12:10:14 (14.1 MB/s) - ‘searchterms.csv’ saved [233457/233457]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download The search term dataset from the below url\n",
        "# https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/searchterms.csv\n",
        "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/searchterms.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAiCDecKIddn",
        "outputId": "2b730115-2926-499d-ed46-106eaa24648f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+--------------+\n",
            "|day|month|year|    searchterm|\n",
            "+---+-----+----+--------------+\n",
            "| 12|   11|2021| mobile 6 inch|\n",
            "| 12|   11|2021| mobile latest|\n",
            "| 12|   11|2021|   tablet wifi|\n",
            "| 12|   11|2021|laptop 14 inch|\n",
            "| 12|   11|2021|     mobile 5g|\n",
            "| 12|   11|2021| mobile 6 inch|\n",
            "| 12|   11|2021|        laptop|\n",
            "| 12|   11|2021|        laptop|\n",
            "| 12|   11|2021|     mobile 5g|\n",
            "| 12|   11|2021|   tablet wifi|\n",
            "| 12|   11|2021|     mobile 5g|\n",
            "| 12|   11|2021| gaming laptop|\n",
            "| 12|   11|2021|     mobile 5g|\n",
            "| 12|   11|2021|     mobile 5g|\n",
            "| 12|   11|2021| mobile 6 inch|\n",
            "| 12|   11|2021| mobile latest|\n",
            "| 12|   11|2021| mobile 6 inch|\n",
            "| 12|   11|2021|   tablet wifi|\n",
            "| 12|   11|2021|     mobile 5g|\n",
            "| 12|   11|2021|        laptop|\n",
            "+---+-----+----+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the csv into a spark dataframe\n",
        "data_dir = \"searchterms.csv\"\n",
        "df = spark.read.csv(data_dir, header=True)\n",
        "df.show()\n",
        "#from pyspark.sql.types import DoubleType\n",
        "#df = df.withColumn(\"x\", df.x.cast(DoubleType()))\n",
        "#df = df.withColumn(\"y\", df.y.cast(DoubleType()))\n",
        "#df = df.withColumn(\"z\", df.z.cast(DoubleType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBcjPEzNIddo",
        "outputId": "dcc36c0d-7ced-4804-c91a-a6d35d2d2840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Rows are: 10000\n",
            "Number of Columns are: 4\n"
          ]
        }
      ],
      "source": [
        "# Print the number of rows and columns\n",
        "# Take a screenshot of the code and name it as shape.jpg\n",
        "row = df.count()\n",
        "   \n",
        "# extracting number of columns from the Dataframe\n",
        "col = len(df.columns)\n",
        "\n",
        "print(f'Number of Rows are: {row}')\n",
        "print(f'Number of Columns are: {col}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GyeZz9iIddp"
      },
      "outputs": [],
      "source": [
        "# register a corresponding query table\n",
        "df.createOrReplaceTempView('df')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcGnK3wHIddp",
        "outputId": "f5f1ce01-a8f4-43e2-ccd6-e517c58b83e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[day: double, month: double, year: double, searchterm: string, features: vector]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_HkqTVtIddp"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import DoubleType\n",
        "df = df.withColumn(\"day\", df.day.cast(DoubleType()))\n",
        "df = df.withColumn(\"month\", df.month.cast(DoubleType()))\n",
        "df = df.withColumn(\"year\", df.year.cast(DoubleType()))\n",
        "#df = df.withColumn(\"searchterm\", df.z.cast(DoubleType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dJozXp9Iddq",
        "outputId": "09baa86c-d0ee-4d27-b748-333b1bde7e9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-----+------+--------------+------------------+\n",
            "| day|month|  year|    searchterm|          features|\n",
            "+----+-----+------+--------------+------------------+\n",
            "|12.0| 11.0|2021.0| mobile 6 inch|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0| mobile latest|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0|   tablet wifi|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0|laptop 14 inch|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0|     mobile 5g|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0| mobile 6 inch|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0|        laptop|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0|        laptop|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0|     mobile 5g|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0|   tablet wifi|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0|     mobile 5g|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0| gaming laptop|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0|     mobile 5g|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0|     mobile 5g|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0| mobile 6 inch|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0| mobile latest|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0| mobile 6 inch|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0|   tablet wifi|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0|     mobile 5g|[12.0,11.0,2021.0]|\n",
            "|12.0| 11.0|2021.0|        laptop|[12.0,11.0,2021.0]|\n",
            "+----+-----+------+--------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "numericCols = ['day', 'month', 'year']\n",
        "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4AXi4oUIddq",
        "outputId": "ac4c63ba-7d9f-4b05-a771-a9f0f3b1b292"
      },
      "outputs": [
        {
          "ename": "IllegalArgumentException",
          "evalue": "'Field \"weight\" does not exist.\\nAvailable fields: day, month, year, searchterm, features'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m~/spark-2.4.3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.3/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o220.fit.\n: java.lang.IllegalArgumentException: Field \"weight\" does not exist.\nAvailable fields: day, month, year, searchterm, features\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:74)\n\tat org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:53)\n\tat org.apache.spark.ml.regression.LinearRegression.org$apache$spark$ml$regression$LinearRegressionParams$$super$validateAndTransformSchema(LinearRegression.scala:176)\n\tat org.apache.spark.ml.regression.LinearRegressionParams$class.validateAndTransformSchema(LinearRegression.scala:119)\n\tat org.apache.spark.ml.regression.LinearRegression.validateAndTransformSchema(LinearRegression.scala:176)\n\tat org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:144)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:100)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_2260/75851666.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlrModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/spark-2.4.3/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m~/spark-2.4.3/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.3/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.3/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/spark-2.4.3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"weight\" does not exist.\\nAvailable fields: day, month, year, searchterm, features'"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "# Create a LR model\n",
        "lr = LinearRegression(featuresCol='features', labelCol='weight', maxIter=100)\n",
        "\n",
        "# Fit the model\n",
        "lrModel = lr.fit(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-oR-9gvIddr"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol = 'class', outputCol = 'labelIndex')\n",
        "df = label_stringIdx.fit(df).transform(df)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8QWtVHwIddr"
      },
      "outputs": [],
      "source": [
        "train, test = df.randomSplit([0.8, 0.2], seed = 1)\n",
        "print(\"Training Dataset Count: \" + str(train.count()))\n",
        "print(\"Test Dataset Count: \" + str(test.count()))\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'labelIndex', maxDepth=7, numTrees=20)\n",
        "rfModel = rf.fit(train)\n",
        "predictions = rfModel.transform(test)\n",
        "predictions.select('x', 'y', 'z', 'labelIndex', 'rawPrediction', 'prediction', 'probability').show(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUddDWuHIddr"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy = %s\" % (accuracy))\n",
        "print(\"Test Error = %s\" % (1.0 - accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qush43L5Idds"
      },
      "outputs": [],
      "source": [
        "# Print the top 5 rows\n",
        "# Take a screenshot of the code and name it as top5rows.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGz8bpLaIdds",
        "outputId": "c3e6d0e8-1e9c-4497-a2d5-7c9fae47f302"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(day=12.0, month=11.0, year=2021.0, searchterm='mobile 6 inch'),\n",
              " Row(day=12.0, month=11.0, year=2021.0, searchterm='mobile latest'),\n",
              " Row(day=12.0, month=11.0, year=2021.0, searchterm='tablet wifi'),\n",
              " Row(day=12.0, month=11.0, year=2021.0, searchterm='laptop 14 inch'),\n",
              " Row(day=12.0, month=11.0, year=2021.0, searchterm='mobile 5g')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23Jbt2URIddt"
      },
      "outputs": [],
      "source": [
        "# Find out the datatype of the column searchterm?\n",
        "# Take a screenshot of the code and name it as datatype.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja7iHV6uIddt",
        "outputId": "a3bc4316-5833-457e-8621-ea0f9847fe26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructField(searchterm,StringType,true)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df.schema[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfKCDStjIddt"
      },
      "outputs": [],
      "source": [
        "# How many times was the term `gaming laptop` searched?\n",
        "# Take a screenshot of the code and name it as gaminglaptop.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aKTpEGNIddu",
        "outputId": "5bca6a98-280d-401b-8b54-3f88d04923e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of times gaming laptop appears in the dataframe: 499\n"
          ]
        }
      ],
      "source": [
        "print(\"The number of times gaming laptop appears in the dataframe: {}\".format(df[df['searchterm']== 'gaming laptop'].count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgxEK_3IIddu"
      },
      "outputs": [],
      "source": [
        "# Print the top 5 most frequently used search terms?\n",
        "# Take a screenshot of the code and name it as top5terms.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbWsLtW_Iddu",
        "outputId": "7abd1d76-1e78-436d-d09c-f7d2288b3c9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|   searchterm|count|\n",
            "+-------------+-----+\n",
            "|mobile 6 inch| 2312|\n",
            "|    mobile 5g| 2301|\n",
            "|mobile latest| 1327|\n",
            "|       laptop|  935|\n",
            "|  tablet wifi|  896|\n",
            "+-------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy('searchterm').count().orderBy('count', ascending=False).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g-78vI_Iddu"
      },
      "outputs": [],
      "source": [
        "# The pretrained sales forecasting model is available at  the below url\n",
        "# https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/model.gzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA_hd25wIddv",
        "outputId": "262cbd1e-7436-4c39-e681-d25152758198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-06 12:10:30--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/model.gzip\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1722 (1.7K) [application/gzip]\n",
            "Saving to: ‘model.gzip’\n",
            "\n",
            "model.gzip          100%[===================>]   1.68K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-03-06 12:10:30 (304 MB/s) - ‘model.gzip’ saved [1722/1722]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/model.gzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D95t0tzrIddv",
        "outputId": "5acbff87-b2f9-4e52-e03a-a020691ebdad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sales_prediction.model/\n",
            "sales_prediction.model/metadata/\n",
            "sales_prediction.model/metadata/part-00000\n",
            "sales_prediction.model/metadata/.part-00000.crc\n",
            "sales_prediction.model/metadata/_SUCCESS\n",
            "sales_prediction.model/metadata/._SUCCESS.crc\n",
            "sales_prediction.model/data/\n",
            "sales_prediction.model/data/part-00000-f37d8b09-cd1a-426c-ba90-4047208b011b-c000.snappy.parquet\n",
            "sales_prediction.model/data/.part-00000-f37d8b09-cd1a-426c-ba90-4047208b011b-c000.snappy.parquet.crc\n",
            "sales_prediction.model/data/_SUCCESS\n",
            "sales_prediction.model/data/._SUCCESS.crc\n"
          ]
        }
      ],
      "source": [
        "!tar -zxvf model.gzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxjcGHJgIddv"
      },
      "outputs": [],
      "source": [
        "# Load the sales forecast model.\n",
        "# Take a screenshot of the code and name it as loadmodel.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIcF8ID8Iddv"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegressionModel\n",
        "model = LinearRegressionModel.load('sales_prediction.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_bGCP_VIddv"
      },
      "outputs": [],
      "source": [
        "# Using the sales forecast model, predict the sales for the year of 2023.\n",
        "# Take a screenshot of the code and name it as forecast.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYdeiVCdIddw"
      },
      "outputs": [],
      "source": [
        "# This function converts a scalar number into a dataframe that can be used by the model to predict.\n",
        "def predict(year):\n",
        "    assembler = VectorAssembler(inputCols=[\"year\"],outputCol=\"features\")\n",
        "    data = [[year,0]]\n",
        "    columns = [\"year\", \"sales\"]\n",
        "    _ = spark.createDataFrame(data, columns)\n",
        "    __ = assembler.transform(_).select('features','sales')\n",
        "    predictions = model.transform(__)\n",
        "    predictions.select('prediction').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wfsX6t7Iddw",
        "outputId": "b7d7a048-e711-44ad-bf60-81fa8a028f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|        prediction|\n",
            "+------------------+\n",
            "|176.14285712605306|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predict(2023)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Spark_MLOps.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}